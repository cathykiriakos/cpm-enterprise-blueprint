# CPM Enterprise Data Platform Blueprint

A comprehensive enterprise data platform blueprint designed for Chicago Public Media, demonstrating modern data engineering practices, constituent unification, and ML-driven engagement strategies.

## ğŸ¯ Executive Summary

This repository provides a production-ready blueprint for unifying constituent data across multiple platforms (WBEZ donations, Sun-Times subscriptions, events) into a single golden record, enabling:

- **360Â° Constituent View**: Unified profiles across all touchpoints
- **Predictive Analytics**: Churn prediction and upgrade propensity models
- **Data Governance**: Single source of truth for all business metrics
- **Multi-Platform Support**: SQL for Standard, Snowflake, and Databricks

## ğŸ“ Repository Structure

```
cpm-enterprise-blueprint/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package installation
â”œâ”€â”€ pyproject.toml           # Modern Python config
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ metrics_definitions.yaml    # Canonical metric definitions
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture/
â”‚       â””â”€â”€ system-overview.md      # Complete architecture documentation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generator.py           # Synthetic data generation
â”‚   â”œâ”€â”€ constituent_unification/
â”‚   â”‚   â””â”€â”€ identity_resolver.py    # Identity resolution engine
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ engine.py               # YAML-driven metrics engine
â”‚   â”œâ”€â”€ ml_models/
â”‚   â”‚   â”œâ”€â”€ churn_prediction.py     # Churn prediction model
â”‚   â”‚   â””â”€â”€ upgrade_propensity.py   # Upgrade propensity model
â”‚   â”œâ”€â”€ integrations/
â”‚   â”‚   â””â”€â”€ base_connector.py       # Data source connector framework
â”‚   â””â”€â”€ data_quality/
â”‚       â””â”€â”€ validator.py            # Data quality validation
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ standard/               # ANSI SQL schemas
â”‚       â”œâ”€â”€ snowflake/              # Snowflake-specific schemas
â”‚       â””â”€â”€ databricks/             # Databricks/Delta Lake schemas
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ notebooks/
â”‚       â””â”€â”€ 01_data_generation_demo.py  # Complete pipeline demo
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ synthetic/                  # Generated test data
    â””â”€â”€ models/                     # Trained ML models
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/ckiriakos/cpm-enterprise-blueprint.git
cd cpm-enterprise-blueprint

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Or install as package
pip install -e .
```

### Generate Synthetic Data

```bash
python src/data_generator.py
```

This creates realistic test data in `data/synthetic/`:
- `wbez_donations.csv` - Donation records (one-time and recurring)
- `suntimes_subscriptions.csv` - Sun-Times subscription data
- `event_tickets.csv` - Event attendance records
- `email_engagement.csv` - Email open/click data
- `ground_truth.csv` - Person-to-source mapping

### Run Identity Resolution

```python
from src.constituent_unification.identity_resolver import (
    IdentityResolver, SourceRecord, ConstituentUnifier
)

# Create source records from your data
records = [SourceRecord(...), ...]

# Unify into golden records
unifier = ConstituentUnifier()
constituents = unifier.unify_records(records)

print(f"Unified {len(records)} records into {len(constituents)} constituents")
```

### Train ML Models

```python
from src.ml_models.churn_prediction import ChurnPredictor, generate_sample_data

# Generate training data
df, labels = generate_sample_data(5000)

# Train model
model = ChurnPredictor()
metrics = model.train(df, labels)
print(f"AUC: {metrics['auc']:.3f}")

# Save model
model.save('data/models/churn_model.pkl')
```

### Generate Metric SQL

```python
from src.metrics.engine import MetricsEngine, SQLPlatform

engine = MetricsEngine('config/metrics_definitions.yaml')

# Get SQL for Snowflake
sql = engine.get_sql('active_member', SQLPlatform.SNOWFLAKE)
print(sql)
```

## ğŸ—ï¸ Architecture Overview

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WBEZ Donations â”‚     â”‚ Sun-Times Subs  â”‚     â”‚  Event Tickets  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   IDENTITY RESOLUTION   â”‚
                    â”‚  (Deterministic + Fuzzy)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     GOLDEN RECORD       â”‚
                    â”‚   (Unified Constituent) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Churn Predictionâ”‚   â”‚ Upgrade Propensity  â”‚   â”‚    Metrics    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Identity Resolution

Two-phase matching algorithm:
1. **Deterministic**: Exact match on email or phone (confidence: 1.0)
2. **Probabilistic**: Weighted fuzzy match on name, address, etc.

Configurable thresholds:
- `â‰¥ 0.85` â†’ Auto-match
- `0.70-0.85` â†’ Manual review queue
- `< 0.70` â†’ No match

### ML Models

#### Churn Prediction
- **Algorithm**: Gradient Boosting (scikit-learn)
- **Target**: Cancellation or 3+ failed payments within 90 days
- **Features**: Engagement recency, email behavior, payment history, tenure
- **Output**: Score 0-1 with risk tier (low/medium/high/critical)

#### Upgrade Propensity
- **Algorithm**: Multi-target Gradient Boosting
- **Targets**: 
  - One-time â†’ Sustainer
  - Sustainer â†’ Increased amount
  - Any â†’ Major gift ($1000+)
- **Output**: Score per upgrade path + recommended action

### Metrics Framework

YAML-driven metric definitions with:
- Business owner and data steward assignment
- SQL for Standard, Snowflake, and Databricks
- Dimension breakdowns (by source, by segment, etc.)
- Quality checks with severity levels
- Version history and governance metadata

## ğŸ“Š Key Components

### Metrics Definitions (`config/metrics_definitions.yaml`)

```yaml
metrics:
  active_member:
    display_name: "Active Members"
    category: membership
    definition: "Constituents with a donation in the trailing 12 months"
    business_owner: "VP, Membership"
    calculation:
      sql_snowflake: |
        SELECT COUNT(DISTINCT constituent_id)
        FROM golden.constituents
        WHERE last_donation_date >= DATEADD(month, -12, CURRENT_DATE())
```

### Quality Checks

```python
from src.data_quality.validator import DataValidator, get_constituent_checks

validator = DataValidator()
validator.add_checks(get_constituent_checks())

report = validator.validate(df, "constituents")
print(report.summary())
```

## ğŸ› ï¸ Development

### Running Tests

```bash
pytest tests/
```

### Code Style

```bash
black src/
flake8 src/
```

### Building Documentation

```bash
# Generate metric documentation
python -c "
from src.metrics.engine import MetricsEngine
engine = MetricsEngine('config/metrics_definitions.yaml')
engine.export_data_dictionary('docs/metrics_dictionary.md')
"
```

## ğŸ“‹ Use Cases

### 1. Membership Team
- Identify high-churn-risk sustainers for retention outreach
- Find upgrade candidates for sustainer conversion campaigns
- Track active member counts with consistent definitions

### 2. Development Team
- Prioritize major gift prospects using capacity modeling
- Unify donor records across platforms
- Track lifetime value and giving trends

### 3. Marketing Team
- Segment audiences based on engagement scores
- Measure campaign effectiveness with standardized metrics
- Personalize messaging based on unified profiles

### 4. Data Engineering
- Standardized connector framework for new data sources
- Quality checks built into pipelines
- Multi-platform SQL generation

## ğŸ”’ Data Governance

This blueprint embeds governance at every level:

- **Metric Definitions**: Canonical SQL with business owner accountability
- **Data Quality**: Automated checks with severity-based alerting
- **Audit Trail**: Full match history for identity resolution
- **Access Control**: Role-based SQL grants in schema definitions

## ğŸ“š Documentation

- `docs/architecture/system-overview.md` - Complete technical architecture
- `config/metrics_definitions.yaml` - All metric definitions
- `examples/notebooks/` - Runnable demonstrations

## ğŸ‘¤ Author

**Catherine Kiriakos**
- LinkedIn: [linkedin.com/in/catherine-kiriakos](https://linkedin.com/in/catherine-kiriakos)
- Email: cathy.a.kiriakos@gmail.com

---

*Built to demonstrate enterprise data platform capabilities for the Director of Enterprise Systems role at Chicago Public Media.*
